Databricks Inclusion

1.Create a databricks account in azure --> launch the portal as well
https://accounts.azuredatabricks.net/data?account_id=e6a94074-0f30-4e0b-8338-a177fb88de0a
make ur account as the admin so that u can login

2.Create a metastore -- top level container and add the data lake and access connector id
A metastore is the top-level container for catalog in Unity Catalog-- it basically stores info and access details about
ur catalog , schemas(db) tables view and functions

access connector id -- connector which allows dbricks to access our data lake storage
In this we create an access connector and then go to the storage account and add a role storage blob contributor and
we assign this role of storage blob contributor to the access connector ie can contribute to this data lake
and then use the id in databricks metatsore

3. create a compute
4. create external locations for the containers under catalog - firts create a storage credentials
which will allow us to read write to out blob containers

goto external data --> Create a new external location
                       An external location allows you to access your data stored in ADLS

df = spark.read.format('parquet')\
   .option("inferSchema", "true")\
       .load('abfss://bronze@end2endstorage.dfs.core.windows.net/raw_data')

1. Check File Listing
Run the following code to verify whether Spark can list files in the specified path:

python
Copy code
dbutils.fs.ls("abfss://bronze@end2endstorage.dfs.core.windows.net/raw_data")

some of the code
files = dbutils.fs.ls("abfss://bronze@end2endstorage.dfs.core.windows.net/raw_data")
for file in files:
    display([(file.name, file.size) for file in files])


#####################
after loading the data we need to transform the data
     withcoulmn -- will create or do some transformation on an existing column

df = df.withColumn('model_category',split(col('model_id'),'-')[0])

# Another scenario where we will see how many units were sold  for every branch every year
display(df.groupBy('year','branchName').agg(sum('units_sold').alias('total_units_sold'))).sortBy('year','total_units_sold',ascending=[1,0])


df.groupBy('year',"BranchName").agg(sum("Units_Sold").alias("Total_Units_Sold")).sortBy("year",'Total_Units_Sold' ascending=[1,0]).display()


write the data into the silver table
df.write.format('parquet')\
    .mode('append')\
        .option('path','abfss://silver@end2endstorage.dfs.core.windows.net/silver_data')\
        .save();

 query the table

 df = spark.read.parquet("abfss://silver@end2endstorage.dfs.core.windows.net/silver_data");
 display(df)

 till here we have done some transformations on the table --> like we have addes 2 new fields

 lets create an incremenatl flag which will determine whether this is the first run or second run

 dbutils.widgets.text('incremental_flag','0')


 Whatâ€™s Happening?
 1. Create a DataFrame (df_sink) Using a SQL Query
 The SQL query:

 sql
 SELECT 1 AS dim_model_key, model_id, model_Category
 FROM silver_data_temp_view
 WHERE 1=0

 SELECT 1 AS dim_model_key: This creates a column called dim_model_key and assigns it the constant value 1 (this is mostly placeholder logic for now).
 FROM silver_data: Indicates the data source (likely a table or a temporary view named silver_data).
 WHERE 1=0: This condition always evaluates to false, meaning no rows will be selected from the silver_data_temp_view table.
 As a result, the query creates an empty DataFrame with the schema derived from silver_data but no rows.

 2. Why Create an Empty Sink (df_sink)?
 Creating an empty DataFrame like this serves as a template or destination schema for writing transformed data later in the pipeline.

 Schema Definition: This ensures that the df_sink DataFrame has the same schema (column names and types) as the expected destination table.
 Pipeline Placeholder: It's common in ETL or ELT pipelines to define the "sink" (destination) structure upfront, even before processing any actual data.
 Dynamic Loading: In some workflows, you might dynamically populate df_sink later with real data that conforms to this schema.
 3. Why Use dim_model_key = 1?
 The dim_model_key is likely a surrogate key for a dimension table in a data warehouse (often used in star schema design).
 By initializing the key with a placeholder value (1), it creates the structure for handling unique identifiers when actual data is loaded.
 4. Using display(df_sink)
 The display() function in Databricks shows the content of the df_sink DataFrame.
 Since WHERE 1=0 ensures no rows are returned, the display will show an empty table with column names: dim_model_key, model_id, model_Category.
 Use Case Example
 This approach is common when preparing staging tables or dimension tables for a data warehouse.

 Example:
 Suppose you have silver_data containing raw or semi-processed data:

 model_id	model_Category
 M001	Electronics
 M002	Automotive
 By creating df_sink, you are setting up the schema for a dimension table like this:

 dim_model_key	model_id	model_Category
 Placeholder (1)	None	None
 Later, you can populate this df_sink with actual data and assign proper surrogate keys dynamically.

 Conclusion
 This code creates an empty DataFrame with a predefined schema and placeholder logic, which acts as a template or sink for future data processing steps. It's useful in pipelines where schema consistency or ETL workflows require pre-defined structures before loading actual data.


