
What is Apache Spark?
Apache Spark is an open-source distributed computing system designed for processing large datasets quickly and efficiently.
It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.

Example: Word Count in Apache Spark
Let’s calculate the number of times each word appears in a text file.

Python Example (PySpark)
Input Data (sample.txt):
Apache Spark is fast.
Spark is easy to use.
Spark powers big data applications.


code

from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "WordCount")

# Load the text file into an RDD (Resilient Distributed Dataset)
text_file = sc.textFile("sample.txt")

# Perform transformations and actions
word_counts = (text_file
               .flatMap(lambda line: line.split())  # Split lines into words
               .map(lambda word: (word, 1))        # Map each word to (word, 1)
               .reduceByKey(lambda a, b: a + b))   # Reduce by key to count occurrences

# Collect and print the results
for word, count in word_counts.collect():
    print(f"{word}: {count}")

 output
 Apache: 1
 Spark: 3
 is: 2
 fast.: 1
 easy: 1
 to: 1
 use.: 1
 powers: 1
 big: 1
 data: 1
 applications.: 1

RDD: The text file is loaded into a distributed dataset (RDD).

Transformations:
flatMap: Splits each line into words.
map: Maps each word to a key-value pair (word, 1).
reduceByKey: Aggregates word counts by key.

Action
collect: Retrieves the results from the distributed environment.

When to Use Apache Spark?
Big Data Processing:Analyzing logs, sensor data, or transactional data.
Real-Time Analytics:Streaming data from IoT devices, web servers, or social media.
Machine Learning:Training models on large datasets using Spark MLlib.
ETL Pipelines:Extracting, transforming, and loading data in a distributed environment.

Spark is the engine for big data processing—fast, scalable, and versatile.

=====================*======
Key Data Objects in Spark
RDD (Resilient Distributed Dataset):

Fundamental low-level data structure.
Immutable, distributed collection of objects.
Harder to use, requires manual optimizations.

DataFrame:
Higher-level abstraction over RDDs.
Tabular format with named columns (like a table in SQL).
Optimized for structured and semi-structured data.
Preferred for most use cases (e.g., querying, analysis).

Important to Remember
Use DataFrames over RDDs for ease of use and performance.
Spark SQL is ideal for querying structured data.
Spark Streaming processes data in real-time.
MLlib and GraphX extend Spark for specialized tasks.

Apache Spark Ecosystem
Component	            Description	                                Use Case

Spark SQL	        SQL-like queries on structured data	            Data analysis and reporting
Spark Streaming     Processes real-time data streams	            Real-time analytics
MLlib	            Machine learning library	                    Scalable machine learning
GraphX	            Library for graph processing	                Social network analysis
Spark Core	        Foundation of Spark; rarely used directly	     Low-level operations

Key Takeaway: Use Spark SQL with DataFrames for structured data processing and analytics.
It's easy, efficient, and the most widely adopted feature of Spark.