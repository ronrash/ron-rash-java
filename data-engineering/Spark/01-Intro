what is apache spark ??
a distributed computing platform

what do we do with apache spark ??
we create programs and execute them on spark clusters

how to create spark programs?
Ultimately we will be building full blown spark applications like stream processing or batch processing
eg read a news feed as a continuous stream --> then apply machine-learning algorithms to figure out what type of users
are interested in what new s and redirect the story to those users

or
For example, YouTube statistics.
In this case,we collect the data for 24 hours and start a scheduled spark job to compute the watch-time minutes for the last 24 hours.
Finally,the outcome goes to a table and also appears on the dashboard.

in both these jobs you must package your application and submit it to the Spark cluster for execution.
and that's the second method for executing your programs on a Spark Cluster.

1.so we can run spark applications on notebooks or interactive ui for learning or development
2. a real-life production implementation is all about packaging your Spark application and submitting it to the cluster for execution.

Spark-submit is the most commonly used tool for this purpose.others are databricks notebooks and rest apis
In databricks cloud they allow u to submit ur notebooks


=====================Spark distributed Processing model ==========




What is Apache Spark?
Apache Spark is an open-source distributed computing system designed for processing large datasets quickly and efficiently.
It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.

Apache Spark does not offer Cluster Management and Storage Management.
Apache Spark is used to run your data processing workload.
And that part is managed by the Spark Compute Engine.
So the compute engine is responsible for a bunch of things.

The compute engine for spark manages
breaking ur data processing work into smaller tasks
scheduling those tasks on the cluster for parallel execution, providing data to these tasks,
managing and monitoring tasks, prvide fault tolerance when a job fails

The core engine is also responsible for interacting with the cluster and storage manager

The secon part of the spark core is the apis
it is the programming interface that offers you the core apis in 4 languages
java scala python and r

The topmost layer
Spark sql and dataframes -- deal with sql queries and dataframse -- for structured unstructured and semistructured data
Streaming - they allow you to process a continuous and unbounded stream of data
mlib -- machine learning libraries
graphx --> graph processing libraires

Why sparki great ?
Spark will abstract away that you are coding to execute your program on a cluster of computers.
You will feel like working with a database.
u will be working with rdds or datframes
it takes away all the complexities of distributed computing
combines sql queries , batch processing

Example: Word Count in Apache Spark
Let’s calculate the number of times each word appears in a text file.

Python Example (PySpark)

Input Data (sample.txt):
Apache Spark is fast.
Spark is easy to use.
Spark powers big data applications.


code

from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "WordCount") -this is the file

# Load the text file into an RDD (Resilient Distributed Dataset)
text_file = sc.textFile("sample.txt")

# Perform transformations and actions
word_counts = (text_file
               .flatMap(lambda line: line.split())  # Split lines into words
               .map(lambda word: (word, 1))        # Map each word to (word, 1)
               .reduceByKey(lambda a, b: a + b))   # Reduce by key to count occurrences

# Collect and print the results
for word, count in word_counts.collect():
    print(f"{word}: {count}")

 output
 Apache: 1
 Spark: 3
 is: 2
 fast.: 1
 easy: 1
 to: 1
 use.: 1
 powers: 1
 big: 1
 data: 1
 applications.: 1

RDD: The text file is loaded into a distributed dataset (RDD).

Transformations:
flatMap: Splits each line into words.
map: Maps each word to a key-value pair (word, 1).
reduceByKey: Aggregates word counts by key.

Action
collect: Retrieves the results from the distributed environment.

When to Use Apache Spark?
Big Data Processing:Analyzing logs, sensor data, or transactional data.
Real-Time Analytics:Streaming data from IoT devices, web servers, or social media.
Machine Learning:Training models on large datasets using Spark MLlib.
ETL Pipelines:Extracting, transforming, and loading data in a distributed environment.

Spark is the engine for big data processing—fast, scalable, and versatile.

=====================*======
Key Data Objects in Spark
RDD (Resilient Distributed Dataset):

Fundamental low-level data structure.
Immutable, distributed collection of objects.
Harder to use, requires manual optimizations.

DataFrame:
Higher-level abstraction over RDDs.
Tabular format with named columns (like a table in SQL).
Optimized for structured and semi-structured data.
Preferred for most use cases (e.g., querying, analysis).

Important to Remember
Use DataFrames over RDDs for ease of use and performance.
Spark SQL is ideal for querying structured data.
Spark Streaming processes data in real-time.
MLlib and GraphX extend Spark for specialized tasks.

Apache Spark Ecosystem
Component	            Description	                                Use Case

Spark SQL	        SQL-like queries on structured data	            Data analysis and reporting
Spark Streaming     Processes real-time data streams	            Real-time analytics
MLlib	            Machine learning library	                    Scalable machine learning
GraphX	            Library for graph processing	                Social network analysis
Spark Core	        Foundation of Spark; rarely used directly	     Low-level operations

Key Takeaway: Use Spark SQL with DataFrames for structured data processing and analytics.
It's easy, efficient, and the most widely adopted feature of Spark.