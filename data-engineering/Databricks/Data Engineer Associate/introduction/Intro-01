HDFS - hadoop distributed file system -> storing large amounts of data across multiple servers/machines
       breaks data into blocks for high availabilty and reliabilty.


MapReduce --> Parallel processing of these data -- to get the final result


Hadoop issues -- hadoop relies on disk based storage,ie. high disk i/o operation slow process.
                 Apache spark is in memory processing framework.
                 coding in hadoop is difficult


Apache Spark -- In memory processing, easy to use , sdk with java scala SQl, 100x faster than  MapReduce
                Apis in multiple langauage


WHat is databricks ?
Databricks is a multi cloud lakehouse platform based on Apache Spark.
It is an analytics platform for data analysis,engineering,science and machine learning workflows
Web platform for Apache spark

Databricks is a cloud-based platform specifically designed for big data processing and analytics.
It builds on Apache Spark and simplifies cluster setup, scalability, and management.
It also supports advanced features like Delta Lake and Lakehouse architecture,
enabling faster and more efficient data processing.



what is a lake house ?
A data lakehouse is a unified analytics platform that combines the best elements of data lakes and data warehouses.
Data lake           +           Data Warehouse
raw data                         refined structured data,
Large                            small ,
data engineers and scientists    relational  data for business use


Web platform for Apache spark
Has many apis for diff programming languages

Architecture of Databricks Lakehouse
                            Workspace
Data Engineering        Data Warehousing        Machine Learning

                       runtime
   Apache Spark                 delta lake

   Cloud Service
   Aws azure gcp



Data LakeHouse Platform
manage large sets of data with acid and perform analysis machine larening and scine

Workspace for Databricks or lakehouse platfom
MachineLearning - platform for building,training deploying machine learning modules
                  Ml Practitioners and Data Scinetists
Databricks Sql -  Platform for quering data in data lake house platforms
                  run queries,visualize data , perform analysis task.
                  Used By data Analysts and Sql Developers
Data Science and Engineering - data engineers/scientist can perform building pipelines develop science models



Databricks uses the infrastructure of your cloud provider to provision virtual machines or nodes of
a cluster.

There are two high level components: the control plane and the data plane .
The control plane resides in Databricks account while the data plane is in your own cloud subscription.

Whenever you create a databricks workspace, it is deployed in the control plane along with Databricks
services like Databricks UI, Cluster Manager, workflow service and notebooks.

Control Plane          vs               Data Plane is ur cloud account
Web Ui                                Cluster Vms {spark cluster is }
Databricks Workspace                 Storage account is in data plane use for dbfs --databricks file system
Wrokflow Service
Notebooks

So to summarize, the compute and the storage will be always in your own cloud account.
Databricks will provide you with the tools you need to use and control your infrastructure.
The data is distributed and processed in-memory of multiple nodes in a cluster.
Databricks supports all languages R java scala python , batch processing

Apache spark processes data in distributed manner , databrikcs offers a native support
So whenever you create a cluster in Databricks, it comes pre-installed with DBFS.

If you create a file in your cluster and store it in DBFS.
This file is actually persisted in the underlying cloud storage.
Like your Azure storage or your S3 buckets.
So even after the cluster is terminated, all the data is safe in your cloud storage.

The control plane is responsible for managing configurations like clusters, notebooks, job scheduling, and the user interface.
 It acts as the central controller, abstracting away infrastructure complexity, allowing users to focus on data processing tasks.
