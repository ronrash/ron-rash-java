HDFS - hadoop distributed file system -> storing large amounts of data across multiple servers/machines
       breaks data into blocks for high availabilty and reliabilty.


MapReduce --> Parallel processing of these data -- to get the final result


Hadoop issues -- hadoop relies on disk based storage,ie. high disk i/o operation slow process.
                 Apache spark is in memory processing framework.
                 coding in hadoop is difficult


Apache Spark -- In memory processing, easy to use , sdk with java scala SQl, 100x faster than  MapReduce
                Apis in multiple langauage


WHat is databricks ?
Databricks is a multi cloud lakehouse platform based on Apache Spark.

what is a lake house

A data lakehouse is a unified analytics platform that combines the best elements of data lakes and data warehouses.
Data lake           +           Data Warehouse
raw data                        refined structured data,
Large                            small ,
data engineers and scientists    relational  data for business use

It is an analytics platform for data analysis,engineering,science and machine learning workflows
Web platform for Apache spark
Has many apis for diff programming languages

Architecture of Databricks Lakehouse
                            Workspace
Data Engineering        Data Warehousing        Machine Learning

                       runtime
   Apache Spark                 delta lake

   Cloud Service
   Aws azure gcp



Data LakeHouse Platform
manage large sets of data with acid and perform analysis machine larening and scine

Workspace for Databricks or lakehouse platfom
MachineLearning - platform for building,training deploying machine learning modules
                  Ml Practitioners and Data Scinetists
Databricks Sql -  Platform for quering data in data lake house platforms
                  run queries,visulize data , perform analsis task.
                  Used By data Analysts and Sql Developers
Data Sciene and Engineering - data engineers/scientist can perform building pipelines develop science models



Databricks uses the infrastructure of your cloud provider to provision virtual machines or nodes of
a cluster.

There are two high level components: the control plane and the data plan .
The control plane resides in Databricks account while the data plane is in your own cloud subscription.

Whenever you create a databricks workspace, it is deployed in the control plane along with Databricks
services like Databricks UI, Cluster Manager, workflow service and notebooks.

Control Plane          vs               Data Plane is ur cloud account
Web Ui                               Cluster Vms {spark cluster is }
Databricks Workspace                 Storage account is in data plane use for dbfs --databricks file system
Wrokflow Service
Notebooks

So to summarize, the compute and the storage will be always in your own cloud account.
Databricks will provide you with the tools you need to use and control your infrastructure.
The data is distributed and processed in-memory of multiple nodes in a cluster.
Databricks supports all languages R java scala python , batch processing

Apache spark processes data in distributed manner , databrikcs offers a native support
So whenever you create a cluster in Databricks, it comes pre-installed with DBFS.

If you create a file in your cluster and store it in DBFS.
This file is actually persisted in the underlying cloud storage.
Like your Azure storage or your S3 buckets.
So even after the cluster is terminated, all the data is safe in your cloud storage.

